{
  # ##########my##########
  #   "world_size": 1,
  #   "num_gpus": 8,
  # ######################

  # parallelism settings
  "pipe-parallel-size": 1,
  "model-parallel-size": 1,

  # model settings
  "num-layers": 12,
  "hidden-size": 768,
  "num-attention-heads": 12,
  "seq-length": 2048,
  "max-position-embeddings": 2048,
  "pos-emb": "rotary",
  # "pos-emb": "alibi",
  "rotary-pct": 0.25,
  "no-weight-tying": true,
  "gpt-j-residual": true,
  "output-layer-parallelism": "column",
  "attention-config": [[["global"], 12]], 

  "scaled_masked_softmax_fusion": true, ####my !!!! For SepLLM
  "bias-gelu-fusion": true,

  # init methods
  "init_method": "small_init",
  "output_layer_init_method": "wang_init",

  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.0006,
      "betas": [0.9, 0.95],
      "eps": 1.0e-8
    }
  },
  "min_lr": 0.00006,

  "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 500000000,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 500000000,
    "contiguous_gradients": true,
    "cpu_offload": false
  },

  # batch size (trained on 32 gpus)
  "train_micro_batch_size_per_gpu": 64, #32 
  "gradient_accumulation_steps": 2,  #4
  "data-impl": "mmap",
  "num_workers": 1,

  # activation checkpointing
  "checkpoint-activations": true,
  "checkpoint-num-layers": 1,
  "partition-activations": true,
  "synchronize-each-layer": true,

  # regularization
  "gradient_clipping": 1.0,
  "weight-decay": 0.1,
  "hidden-dropout": 0,
  "attention-dropout": 0,

  # precision settings
  "fp16": {
    "fp16": true,
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000,
    "initial_scale_power": 12,
    "hysteresis": 2,
    "min_loss_scale": 1
  },

  "train-iters": 143000,
  "lr-decay-iters": 143000,
  "distributed-backend": "nccl",
  "lr-decay-style": "cosine",
  "warmup": 0.01,
  "checkpoint-factor": 1000,
  "extra-save-iters": [0,1,2,4,8,16,32,64,128,256,512],
  "eval-interval": 4000, 
  "eval-iters": 10,

  "log-interval": 10,
  "steps_per_print": 10,
  "wall_clock_breakdown": true,

  "train-data-paths": ["path/to/pile_0.87_deduped_text_document"],
  "valid-data-paths": ["path/to/pile_0.87_deduped_text_document"],
  "test-data-paths":  ["path/to/pile_0.87_deduped_text_document"],

  "tokenizer-type": "HFTokenizer",
  "vocab-file": "path/to/SepLLM/Training-SepLLM/sample_configs/20B_tokenizer.json",

  "launcher": "pdsh",

  ## SepLLM:
  ####################################################################### SepLLM #######################################################################:
  # "hostfile": "path/to/hostfile",

  'separator_token_ids': [15, 13, 32, 2, 28, 27, 209, 186, 187],  ## Fixed
  'PADDING_ID': 0 , # For pythia  ## Fixed
  
  'prefill_local_window_size' : 64,  # The local window size when prefilling (tokens inside the window are kept);  Only take effect when USE_DYNAMIC_PREFILL_WINDOW_SIZE=False
  'generate_local_window_size' : 64,   # The local window size when generating (tokens inside the window are kept);  Only take effect when USE_DYNAMIC_DECODE_WINDOW_SIZE=False        

  'USE_PREFILL_LOCAL_WIN_SIZES_wrt_LAYERS'  :   False,  # If True: the prefilling window sizes for different decoder layers are different; If True: should set 'prefill_win_size_list', else: should set 'prefill_window_size'
  'USE_GENERATE_LOCAL_WIN_SIZES_wrt_LAYERS' :   False,  # If True: the decoding window sizes for different decoder layers are different;   If True: should set 'decode_win_size_list', else: should set 'decode_window_size' 
  

  'prefill_loc_win_size_list' :   [2048,   64,   640,   2048,   64,   2048,   720,   320,   64,   64, 
                             64,   2048],  
                                 

  'generate_win_loc_size_list':   [64,   64,   64,   64,   64,   64,   64,   64,   64,   64, 
                             64,   64],  
                                                    

  'init_tok_max_idx' :  2, # The largest index for attention sink tokens

  
  'USE_BiPE':  False,  # If True (must also set pos_emb='rotary' or 'alibi'), use Bilevel Positional Encoding.
  'BiPE_seps': [ 15, 13, 32, 2, 28, 27, 209, 186, 187 ],  # The token ids of the seperator tokens for BiPE:  ['.', ',', '?', '!', ';', ":", ' ', '\t','\n'] 
  ######################################There should be at most 1 True for the following 3 args ##############################################
  'USE_ORIGINAL_FULL_ATTEN' : False,  # Train the model without SepLLM's modification if True. Other modifications depend on other parameters' setting such as USE_SA_SOFTMAX.
  'streamingLLM' :  False,  ## NOT implemented yet  # Run streamingLLM. Only takes effect when self.original_flag=False
  'USE_SEP_ATTN_KERNEL_ACCELERATOR': True, # If True, use Flex-attention.
  ######################################There should be at most 1 True for the above 3 args ##############################################

  'BATCH_ADAPTIVE_INIT_POS' : True,  # If True: use the floating attension sink positions since when evaluating, LLM usually add paddings on the left side of the shorter seqs in a batch for alignment (i.e., left padding).  Can be False when pretraining since the attention sinks are at the beginning of each seq in batch for pretraining (i.e., right padding)
  'PRINT_KV_RATIO' :  True,  # If True, print the KV cache preservation ratio (especially for generating). When pretraining, it will print the retention ratio for the computational complexity of calculating the attention map if it is set True
  'print_ratio_intervals': 8000,   # Print the retention ratio for the computational complexity of calculating the attention map once after every 'print_KV_intervals' forward passes (or print_KV_intervals/gradient_accumulation_steps  iterations). It only takes effect when PRINT_KV_RATIO=True.    
  ####################################################################### ### #######################################################################
}
